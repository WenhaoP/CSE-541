{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE 541 Homework 1\n",
    "Evan Komp\n",
    "***\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1\n",
    "Prove Markov's inequality eg $P(X > \\lambda)\\le \\mathbb{E}(X)/\\lambda$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition of the expected value, for a positive random variable $X$:\n",
    "\n",
    "$$\\mathbb{E}(X) = \\int_0^\\inf xf(x)dx$$\n",
    "\n",
    "Due to the law of total probability, $f(x)\\ge 0$:\n",
    "\n",
    "$$\\ge \\int_\\lambda^\\inf xf(x)dx$$\n",
    "\n",
    "Given that $X$ is real, positive, and greater than lambda:\n",
    "\n",
    "$$\\ge \\lambda\\int_\\lambda^\\inf f(x)dx$$\n",
    "\n",
    "By definition of the cummaltive distributio. function\n",
    "> $$\\ge \\lambda P(X>\\lambda)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a random vector $X\\in \\mathbb{R}^d$ with convex function $\\phi: \\mathbb{R}^d \\rightarrow \\mathbb{R}$. Show for discrete $X$ that $\\phi(\\mathbb{E}(X)) \\le \\mathbb{E}(\\phi(X))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have the definition of convexity for $x_i\\in X$: $\\phi(tx_i+(1-t)x_j)\\le t\\phi(x_i)+(1-t)\\phi(x_j)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our expected value $\\mathbb{E}(X)=\\Sigma_i^n x_i p(x_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the law of total probability:\n",
    "$$\\phi(\\Sigma_{i=1}^nx_ip(x_i)) = \\phi(x_1p(x_1)+(1-p(x_1))\\frac{\\Sigma_{i=2}^nx_ip(x_i)}{\\Sigma_{i=2}^np(x_i)})$$\n",
    "\n",
    "Given that the function is convex:\n",
    "\n",
    "$$\\le p(x_1)\\phi(x_1)+(1-p(x_1))\\phi(\\frac{\\Sigma_{i=2}^nx_ip(x_i)}{\\Sigma_{i=2}^np(x_i)})$$\n",
    "\n",
    "Given that the function maps a vector of length d to a scalar, we have the scalar denominator as independant:\n",
    "$$\\le x_1p(x_1)+\\frac{1-p(x_1)}{\\Sigma_{i=2}^np(x_i)}\\Sigma_{i=2}^nx_ip(x_i) = x_1p(x_1)+\\Sigma_{i=2}^nx_ip(x_i)$$\n",
    "\n",
    "Repeating n times yields:\n",
    "\n",
    ">$$\\phi(\\mathbb{E}(X))\\le \\phi(x_1)p(x_1)+,...,+x_np(x_n) = \\mathbb{E}(\\phi(X))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3\n",
    "\n",
    "If $X_i$ are independant random sub gaussian variables with $\\mathbb{E}[\\exp(\\lambda(X_i - \\mathbb{E}(X_i)))]\\le \\exp(\\lambda^2\\sigma^2/2)$ for $\\lambda \\gt 0$, and $Z=\\Sigma X_i$\n",
    "\n",
    "By identity:\n",
    "$$\\mathbb{E}[\\exp(\\lambda(Z-a))] = \\mathbb{E}[\\Pi_i\\exp(\\lambda X_i)\\exp(-\\lambda a)]=\\exp(\\lambda(\\Sigma_i\\mathbb{E}(X_i)-a))\\mathbb{E}[\\Pi_i\\exp(\\lambda( X_i-\\mathbb{E}(X_i)))]$$\n",
    "\n",
    "By independant random variables:\n",
    "$$=\\exp(\\lambda(\\Sigma_i\\mathbb{E}(X_i)-a))\\Pi_i\\mathbb{E}[\\exp(\\lambda( X_i-\\mathbb{E}(X_i)))]$$\n",
    "\n",
    "By each being sub gaussian:\n",
    "$$\\le \\exp(\\lambda(\\Sigma_i\\mathbb{E}(X_i)-a))\\Pi_i \\exp(\\lambda^2\\sigma_i^2/2)$$\n",
    "\n",
    "So our target bound becomes: \n",
    "$$\\exp(\\lambda(\\Sigma_i\\mathbb{E}(X_i)-a))\\Pi_i \\exp(\\lambda^2\\sigma_i^2/2)\\le \\exp(\\lambda^2b/2)$$\n",
    "\n",
    "Taking the log and simplifying:\n",
    "$$\\lambda\\frac{b-\\Sigma\\sigma_i^2}{2} - \\Sigma\\mathbb{E}(X_i)+a \\le 0$$\n",
    "\n",
    "This must be true for all $\\lambda > 0$, thus taking $\\lim_{\\lambda\\rightarrow 0}$ and to infinity yields. These are non interacting loose bounds:\n",
    "\n",
    "$$a \\le \\Sigma\\mathbb{E}(X_i)$$\n",
    "$$b \\ge \\Sigma_i\\sigma_i^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4\n",
    "If $X_i$ with $i=1,...,n$ be a sub gaussian random variable with $\\mathbb{E}(\\exp(\\lambda X_i))\\le \\exp(\\sigma_i^2\\lambda^2/2)$ what is the upper bound on $\\mathbb{E}(\\max_iX_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting from the identity given as a hint:\n",
    "$$\\mathbb{E}(\\max_iX_i)=\\frac{1}{\\lambda}\\ln(\\exp(\\lambda\\mathbb{E}[\\max_iX_i]))$$\n",
    "\n",
    "And by Jensen's inequality on the exponentiation function:\n",
    "$$\\le \\frac{1}{\\lambda}\\ln(\\mathbb{E}[\\exp(\\lambda\\max_iX_i)])$$\n",
    "\n",
    "The max cannot be more than the sum:\n",
    "$$\\le \\frac{1}{\\lambda}\\ln(\\mathbb{E}[\\exp(\\lambda\\Sigma_iX_i)]) = \\frac{1}{\\lambda}\\ln(\\mathbb{E}[\\Pi_i\\exp(\\lambda X_i)])$$\n",
    "\n",
    "Independance:\n",
    "$$= \\frac{1}{\\lambda}\\ln(\\Pi_i\\mathbb{E}[\\exp(\\lambda X_i)])$$\n",
    "\n",
    "Sub Gaussian:\n",
    "$$\\le \\frac{1}{\\lambda}\\ln(\\Pi_i\\exp(\\sigma_i^2\\lambda^2/2))$$\n",
    "\n",
    "A product of n numbers is at most the product of the maximum number n times:\n",
    "$$\\le \\frac{1}{\\lambda}\\ln(n\\exp(\\max_i\\sigma_i^2\\lambda^2/2))$$\n",
    "$$=\\frac{\\ln(n)}{\\lambda}+\\max_i\\sigma_i^2\\lambda/2$$\n",
    "\n",
    "Minimize w.r.t $\\lambda$:\n",
    "$$\\lambda \\ge \\sqrt{\\frac{\\ln(n)2}{\\max_i\\sigma_i^2}}$$\n",
    "\n",
    "Thus:\n",
    "$$\\mathbb{E}(max_iX_i) \\le \\sqrt{2\\max_i\\sigma_i^2\\ln(n)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Upper Confidence Bound Algortithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1\n",
    "\n",
    "For UCB with event defined as\n",
    "$$\\mathcal{E}=\\bigg\\{\\bigcap_{i\\in[n]}\\bigcap_{s\\le T} |\\hat{\\mu}_{i,s} - \\mu_i| \\le \\sqrt{\\frac{2\\ln{2nT^2}}{s}}\\bigg\\}$$\n",
    "\n",
    "Show that $\\mathbb{P}(\\mathcal{E}) \\ge 1 - T^{-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have\n",
    "$$\\mathbb{P}(\\mathcal{E}) = 1-\\mathbb{P}(\\mathcal{E})^c= 1 - \\bigcup_{i\\in[n]}\\bigcup_{s\\le T} \\mathbb{P}\\bigg(\\mathbb{1}\\bigg\\{|\\hat{\\mu}_{i,s} - \\mu_i| \\ge \\sqrt{\\frac{2\\ln{2nT^2}}{s}}\\bigg\\}\\bigg)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the chernoff bound for 1-sub-gaussian variables:\n",
    "\n",
    "$$\\ge 1 - \\bigcup_{i\\in[n]}\\bigcup_{s\\le T}2\\exp{\\frac{-T_i(s)\\sqrt{\\frac{2\\ln{2nT^2}}{s}}^2}{2}} = 1 - \\bigcup_{i\\in[n]}\\bigcup_{s\\le T}2\\exp{\\frac{-T_i(s)\\ln{2nT^2}}{s}}$$\n",
    "\n",
    "Union bound, and the most that $T_i(s)$ could be is $s+1-n \\approx s$ for large s, yields:\n",
    "\n",
    "$$\\ge 1 - \\Sigma_{i\\in[n]}\\Sigma_{s\\le T}\\frac{1}{nT^2}$$\n",
    "> $$\\ge 1-T^{-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 2.2\n",
    "Show the bound for $T_i$\n",
    "\n",
    "We have the following condition for arm $i$ being pulled:\n",
    "\n",
    "$$max_j\\bigg(\\hat{\\mu}_{j,s-1} + \\sqrt{\\frac{2\\ln{2nT^2}}{T_{j,s-1}}}\\bigg) =\\hat{\\mu}_{i,s-1} + \\sqrt{\\frac{2\\ln{2nT^2}}{T_{i,s-1}}}$$\n",
    "\n",
    "The max of a sum cannot be more than the sum of the maxes:\n",
    "\n",
    "$$max_j\\bigg(\\hat{\\mu}_{j,s-1}\\bigg) + max_j\\bigg(\\sqrt{\\frac{2\\ln{2nT^2}}{T_{j,s-1}}}\\bigg) \\le \\hat{\\mu}_{i,s-1} + \\sqrt{\\frac{2\\ln{2nT^2}}{T_{i,s-1}}}$$\n",
    "\n",
    "The largest empirical mean is the first one, adn the most that $T_{j,s-1}$ can be is $s-n\\approx s$\n",
    "\n",
    "$$\\hat{\\mu}_{1,s-1} + \\sqrt{\\frac{2\\ln{2nT^2}}{s}} \\le \\hat{\\mu}_{i,s-1} + \\sqrt{\\frac{2\\ln{2nT^2}}{T_{i,s-1}}}$$\n",
    "\n",
    "By our event:\n",
    "$$\\hat{\\mu}_{i,s-1} - \\mu_i \\le \\sqrt{\\frac{2\\ln{2nT^2}}{s-1}}$$\n",
    "\n",
    "So with $s \\approx s -1$ again:\n",
    "$$-\\sqrt{\\frac{2\\ln{2nT^2}}{s}} + \\Delta_i <\\sqrt{\\frac{2\\ln{2nT^2}}{T_{i,s-1}}}$$\n",
    "\n",
    "At time $s=T$:\n",
    "\n",
    "$$\\Delta_i < (T^{-1}+T_{i,T-1}^{-1})\\sqrt{2\\ln{2nT^2}}$$\n",
    "\n",
    "$T^{-1}\\approx 0$:\n",
    "$$T_{i,T-1} \\le \\frac{2\\ln{2nT^2}}{\\Delta_i^2}$$\n",
    "\n",
    "We cannot pull more than once from arm $i$ in the last timestep:\n",
    ">$$T_i \\le 1+ \\frac{2\\ln{2nT^2}}{\\Delta_i^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3\n",
    "\n",
    "Trivially, if $T_i \\le 1+ \\frac{2\\ln{2nT^2}}{\\Delta_i^2}$, then $\\mathbb{E}(T_i) \\le 1+ \\frac{2\\ln{2nT^2}}{\\Delta_i^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have regret:\n",
    "\n",
    "$$R_T = \\Sigma_i^n\\Delta_i\\mathbb{E}[T_i]$$\n",
    "\n",
    "By total probability:\n",
    "\n",
    "$$= \\Sigma_i^n\\bigg(\\Delta_i\\mathbb{E}[T_i\\mathbb{1}\\{\\mathcal{E}\\}] + \\Delta_i\\mathbb{E}[T_i\\mathbb{1}\\{\\mathcal{E}^c\\}]\\bigg)$$\n",
    "\n",
    "The probability that our event holds is $\\le 1$ and that it doesn't is $\\le \\delta$. Also note that in the case the event happens, we know the upper bound on the expected value of $T_i$. In the case the event happens, the worst it could possibly be is $T-n+1 \\approx T$:\n",
    "\n",
    "$$\\le \\Sigma_i^n\\bigg(\\Delta_i(\\frac{8 \\ln(2nT^2)}{\\Delta_i^2}+2) + \\Delta_iT\\delta\\bigg)$$\n",
    "\n",
    "Choose $\\delta$ s.t.\n",
    "\n",
    "$$\\frac{8\\ln{2nT^2}}{\\Delta_i} + \\Delta_iT\\delta = \\frac{24\\ln(2T)}{\\Delta_i}$$\n",
    "\n",
    "Eg.\n",
    "$$\\delta = \\frac{8\\ln{4T/n}}{\\Delta_i^2T}$$\n",
    "\n",
    "Yielding\n",
    "> $$R_T \\le \\Sigma_i^n \\frac{24\\ln(2T)}{\\Delta_i} + 2\\Delta_i$$\n",
    "\n",
    "Given that $\\Delta_1 = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Thompson Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1\n",
    "\n",
    "The probability of our compliment event:\n",
    "$$\\mathbb{P}(\\mathcal{E}^c) = \\bigcup_i^n\\bigcup_t^T\\mathbb{P}\\bigg(\\mathbb{1}\\bigg\\{|\\hat{\\theta_{i,t}} - \\theta_i^*|\\ge\\sqrt{\\frac{2\\ln{2/\\delta}}{t}}\\bigg\\}\\bigg)$$\n",
    "\n",
    "Given that $\\hat{\\theta_{i,t}} \\in [-1,1]$ it is 1-sub-gaussian by Heoffding. This allows us to apply the Chernoff and union bound:\n",
    "$$\\le \\Sigma_i^n\\Sigma_t^T2\\exp{\\frac{-T_{i,t} (\\frac{2\\ln{2/\\delta}}{t})}{2}}$$\n",
    "\n",
    "$T_{i,t}$ is at most t:\n",
    "\n",
    ">$$\\le \\Sigma_i^n\\Sigma_t^T \\delta = nT\\delta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Empirical testing\n",
    "\n",
    "Agent code and plotting code is shown at the end."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 \n",
    "One arm does slightly better than all of the others\n",
    "\n",
    "![4.1](p4.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I notice that Thompson significantly outperforms UCB. Explore then commit did very well for all m's tested, where it played the best arm into eternum. It is sort of risky though as seen in the plot for 4.2 - if the algorithm misidentifies the best arm it also has infinite regret.\n",
    "\n",
    "### 4.2\n",
    "\n",
    "![4.2](p4.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same observations for n=40 with varying mu, Thompson outperforms. In this case ETC with m = 10 did not identify the correct arm and so has infinite regret. Not that is is __not__ the lowest m value tested, suggesting that m=5 is not a safe choice for this dataset even considering that it identified the best arm in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## Hypothesis testing bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1\n",
    "We have $n$ samples each drawn from $P \\in \\{P_0, P_1\\}$. The function $\\phi(x_1,...,x_n)$ maps data draw to a boolean of the hypothesis $H_1: P = P_1$ or the null $H_0: P = P_0$. The probability density function exists for the distribution such that $p_i(x) = \\Pi_j^np_i(x_j)$\n",
    "\n",
    "The maximum probability of the test ($\\phi$) being False given the true distribution is the hypothesis, or of being True given the true distribution is the null:\n",
    "\n",
    "$$\\inf_\\phi\\max\\{\\mathbb{P}_0(\\phi=1),\\mathbb{P}_1(\\phi=0)\\}$$\n",
    "\n",
    "Is lower bounded by the average:\n",
    "\n",
    "$$\\ge \\inf_\\phi \\frac{\\mathbb{P}_0(\\phi=1)+\\mathbb{P}_1(\\phi=0)}{2} = \\frac{^{\\phi=1}\\int p_0(x_1,...,x_n)dx_1...dx_n+^{\\phi=0}\\int p_1(x_1,...,x_n)dx_1...dx_n}{2}$$\n",
    "\n",
    "The samples arrive iid:\n",
    "\n",
    "$$=\\frac{^{\\phi=1}\\int p_0(x_1)...p_0(x_n)dx_1...dx_n+^{\\phi=0}\\int p_1(x_1)...p_1(x_n)dx_1...dx_n}{2} = \\frac{^{\\phi=1}_{\\mathbb{R}^n}\\int p_0(x)dx+^{\\phi=0}_{\\mathbb{R}^n}\\int p_1(x)dx}{2}$$\n",
    "\n",
    "We can leverage the fact that $A\\ge\\min(A,B)$ for $A,B \\in \\mathbb{R}^n$\n",
    "\n",
    "$$\\ge\\frac{^{\\phi=1}_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx+^{\\phi=0}_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx}{2} = \\frac{_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx}{2}$$\n",
    "\n",
    "> $$\\inf_\\phi\\max\\{\\mathbb{P}_0(\\phi=1),\\mathbb{P}_1(\\phi=0)\\}\\ge \\frac{_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx}{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2\n",
    "\n",
    "We know that $_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx \\le 2$ by total probability, so with this bounded identity:\n",
    "\n",
    "$$_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx \\ge \\frac{_{\\mathbb{R}^n}\\int \\min(p_0(x),p_1(x))dx\\cdot _{\\mathbb{R}^n}\\int \\max(p_0(x),p_1(x))dx}{4}$$\n",
    "\n",
    "By Cauchy-Schwartz:\n",
    "$$\\ge \\frac{1}{4}\\bigg[_{\\mathbb{R}^n}\\int \\sqrt{\\min(p_0(x),p_1(x))\\max(p_0(x),p_1(x))}dx\\bigg]^2 = \\frac{1}{4}\\bigg[_{\\mathbb{R}^n}\\int \\sqrt{p_0(x)p_1(x)}dx\\bigg]^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3\n",
    "By identity:\n",
    "\n",
    "$$\\frac{1}{4}\\bigg[_{\\mathbb{R}^n}\\int \\sqrt{p_0(x)p_1(x)}dx\\bigg]^2 = \\frac{1}{4}\\bigg[\\exp{(2\\ln{(_{\\mathbb{R}^n}\\int p_1(x)\\sqrt{\\frac{p_0(x)}{p_1(x)}}dx)})}\\bigg]$$\n",
    "\n",
    "By Jensen's innequality:\n",
    "\n",
    "$$\\ge \\frac{1}{4}\\bigg[\\exp{(2\\cdot_{\\mathbb{R}^n}\\int p_1(x)\\ln{(\\sqrt{\\frac{p_0(x)}{p_1(x)}})dx})}\\bigg] =  \\ge \\frac{1}{4}\\bigg[\\exp{(-_{\\mathbb{R}^n}\\int p_1(x)\\ln{(\\frac{p_1(x)}{p_0(x)})dx})}\\bigg]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 5.4\n",
    "\n",
    "The gaussian PDF: $p(x) = \\frac{1}{\\sigma2\\pi}\\exp{(-\\frac{(x-\\mu)^2}{2\\sigma^2})}$\n",
    "\n",
    "Plugging into the equation for KL divergence in one dimension:\n",
    "\n",
    "$$KL(p_0, p_1) = \\int p_1(x)\\ln{(\\frac{p_1(x)}{p_0(x)})dx} = \\int\\frac{1}{\\sqrt{I}2\\pi}\\exp{(-\\frac{(x-\\mu_1)^2}{2I})}\\ln{(\\frac{-1}{\\sqrt{I}2\\pi}\\exp{(-\\frac{(x-\\mu_1)^2}{2I}))}} - \\int\\frac{1}{\\sqrt{I}2\\pi}\\exp{(-\\frac{(x-\\mu_1)^2}{2I})}\\ln{(\\frac{-1}{\\sqrt{I}2\\pi}\\exp{(-\\frac{(x-\\mu_0)^2}{2I}))}}$$\n",
    "\n",
    "Integrating the left term and pulling out the right term:\n",
    "\n",
    "$$\\frac{-1}{2}(1+\\ln{2\\pi I}) + \\frac{1}{2}(\\ln{2\\pi I})- \\int\\frac{1}{\\sqrt{I}2\\pi}\\exp{(-\\frac{(x-\\mu_1)^2}{2I})}(-\\frac{(x-\\mu_2)^2}{2I})$$\n",
    "\n",
    "Splitting the integral:\n",
    "$$=\\frac{-1}{2} +\\frac{1}{2I}[\\int p_1(x)x^2dx+\\int p_1(x)\\mu_0^2dx+ \\int p_1(x)2\\mu_0dx]$$\n",
    "\n",
    "$$=\\frac{-1}{2} + \\frac{1}{2I}[\\mathbb{E}_1(x^2) + \\mu_0^2+2\\mu_0\\mathbb{E}_1(x)]$$\n",
    "\n",
    "We know a gaussians squared expected value:\n",
    "$$=\\frac{-1}{2} + \\frac{1}{2I}[I+(\\mu_1-\\mu_0)^2]$$\n",
    "\n",
    "Given that we have $n$ identical distribution samples:\n",
    "$$KL(p_0, p_1) = \\frac{n}{2}(-1 + \\frac{1}{I}[I+(\\mu_1-\\mu_0)^2])$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.5\n",
    "\n",
    "We want to bound the probability of either type of error, eg:\n",
    "\n",
    "$$\\delta \\ge \\inf_\\phi\\max\\{\\mathbb{P}_0(\\phi=1),\\mathbb{P}_1(\\phi=0)\\}$$\n",
    "\n",
    "Based on our previous results we have:\n",
    "$$\\delta \\ge \\frac{1}{4}\\exp{(-nKL(p_0,p_1))}$$\n",
    "\n",
    "$$n\\ge \\frac{\\ln{\\frac{1}{4\\delta}}}{KL(p_0,p_1)}$$\n",
    "\n",
    "For gaussian hypothesis with the same var:\n",
    "$$n\\ge \\frac{\\ln{\\frac{1}{4\\delta}}}{\\frac{1}{2}(-1 + \\frac{1}{I}[I+\\Delta^2])}$$\n",
    "\n",
    ">$$n\\ge \\frac{2I\\ln{\\frac{1}{4\\delta}}}{\\Delta^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "***\n",
    "## Ap. Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Empirical tests of the agents\"\"\"\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from joblib import delayed, Parallel\n",
    "\n",
    "class Agent:\n",
    "    \"\"\"Exploratory agent\n",
    "    \n",
    "    Properties:\n",
    "     T - max time to run\n",
    "     arms - vector of scipy distrobutions\n",
    "     log_regret_every_n - None or int, when to save regret\n",
    "    \n",
    "    Attributes:\n",
    "     T - int, max time\n",
    "     t - time, starts at 0\n",
    "     arms - vector of scipy distrobutions\n",
    "     emp_means - vector of current empirical means for arms\n",
    "     Tis - vector of pull counts for arms\n",
    "     regret - float, current regret\n",
    "     means - vector of true means\n",
    "     true_best - index of true best arm\n",
    "     delts - vector of mean differences from true best arm\n",
    "    \"\"\"\n",
    "    def __init__(self, T: int, arms: List[object], log_regret_every_n: int = None):\n",
    "        self.T = T\n",
    "        self.arms = arms\n",
    "        self.n = len(arms)\n",
    "        self.emp_means = np.zeros((self.n,1))\n",
    "        self.Tis = np.zeros((self.n,1), dtype=int)\n",
    "        self.log_regret_every_n = log_regret_every_n\n",
    "        self.regret = 0.0\n",
    "        self.regret_log = []\n",
    "        self._t = 0\n",
    "        self.startup()\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def t(self):\n",
    "        return self._t\n",
    "    \n",
    "    def startup(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def pull(self, arm_num):\n",
    "        # pull the arm and update the mean for that arm\n",
    "        observation = self.arms[arm_num].rvs()\n",
    "        self.update_emp_mean(arm_num, observation)\n",
    "        # mark that we have pulled\n",
    "        self.Tis[arm_num] += 1\n",
    "        # update regret\n",
    "        self.update_regret(arm_num)\n",
    "        self._t += 1\n",
    "        \n",
    "        # check for logging regret\n",
    "        if self.log_regret_every_n is not None:\n",
    "            if self.t % self.log_regret_every_n == 0:\n",
    "                self.regret_log.append((self.t, self.regret))\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def means(self):\n",
    "        return np.array([arm.mean() for arm in self.arms])\n",
    "    \n",
    "    @property\n",
    "    def true_best(self):\n",
    "        return np.argmax(self.means)\n",
    "    \n",
    "    @property\n",
    "    def delts(self):\n",
    "        best = max(self.means)\n",
    "        return best - self.means\n",
    "    \n",
    "    def update_emp_mean(self, arm_num, observation):\n",
    "        \"\"\"Update empirical for an arm given a new observation\n",
    "        \n",
    "        Properties:\n",
    "         arm_num - int index of arm to update\n",
    "         obervation - float of observation from that arm\n",
    "        \"\"\"\n",
    "        old_mean = self.emp_means[arm_num]\n",
    "        new_mean = (old_mean * self.Tis[arm_num] + observation)/(self.Tis[arm_num] + 1)\n",
    "        self.emp_means[arm_num] = new_mean\n",
    "        return\n",
    "    \n",
    "    def update_regret(self, arm_num):\n",
    "        \"\"\"Update regret by specifying which arm was pulled.\n",
    "        \n",
    "        Properties:\n",
    "         arm_num - int index of arm pulled\n",
    "        \"\"\"\n",
    "        self.regret += self.delts[arm_num]\n",
    "        return\n",
    "    \n",
    "    def step(self):\n",
    "        raise NotImplemented()\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the algorithm until max time\"\"\"\n",
    "        for t in range(self.T-self.t):\n",
    "            self.step()\n",
    "            \n",
    "            \n",
    "class UCB(Agent):\n",
    "    \"\"\"Selects arm with highest upper confidence bound at each step.\"\"\"\n",
    "    def __init__(self, T: int, arms: List[object], log_regret_every_n: int = None):\n",
    "        super().__init__(T, arms, log_regret_every_n)\n",
    "        return\n",
    "    \n",
    "    def startup(self):\n",
    "        \"\"\"Pull each arm once\"\"\"\n",
    "        for i in range(self.n):\n",
    "            self.pull(i)\n",
    "        return\n",
    "    \n",
    "    @property\n",
    "    def ucbs(self):\n",
    "        \"\"\"values of the ucbs, same length as arms\"\"\"\n",
    "        cbs = np.sqrt(2*np.log(2*self.n*self.T**2)/self.Tis)\n",
    "        ucbs = self.emp_means + cbs\n",
    "        return ucbs\n",
    "    \n",
    "    def step(self):\n",
    "        \"\"\"determine the arm with largest ucb and pull it\"\"\"\n",
    "        It = np.argmax(self.ucbs)\n",
    "        self.pull(It)\n",
    "        return\n",
    "    \n",
    "class ETC(Agent):\n",
    "    \"\"\"Pulls each arm m time and then chooses the best arm forever after\"\"\"\n",
    "    def __init__(self, T: int, arms: List[object], m: int,  log_regret_every_n: int = None):\n",
    "        super().__init__(T, arms,  log_regret_every_n)\n",
    "        self.m = m\n",
    "        return\n",
    "    \n",
    "    def startup(self):\n",
    "        # no additional initialization needed\n",
    "        return\n",
    "    \n",
    "    def step(self):\n",
    "        # this class indexex time starting at 0 instead of 1\n",
    "        t_ = self.t+1\n",
    "        if t_ <= self.m*self.n:\n",
    "            It = (t_ % self.n)\n",
    "        else:\n",
    "            It = np.argmax(self.emp_means)\n",
    "        self.pull(It)\n",
    "        return\n",
    "        \n",
    "        \n",
    "class GaussianThompson(Agent):\n",
    "    \"\"\"Assumes posterior distributions for arms are independant.\n",
    "    \n",
    "    Note that the conjugate liklihood to the normal prior is also normal\n",
    "    \"\"\"\n",
    "    def __init__(self, T: int, arms: List[object], prior_mean: float, prior_var: float, log_regret_every_n: int = None):\n",
    "        self.prior_mean = prior_mean\n",
    "        self.prior_var = prior_var\n",
    "        super().__init__(T, arms, log_regret_every_n)\n",
    "        return\n",
    "    \n",
    "    def startup(self):\n",
    "        self.posteriors = [scipy.stats.norm(loc=self.prior_mean, scale=np.sqrt(self.prior_var)) for i in range(self.n)]\n",
    "        return\n",
    "    \n",
    "    def update_posterior(self, arm_num):\n",
    "        old_precision = 1/self.posteriors[arm_num].var()\n",
    "        old_mean = self.posteriors[arm_num].mean()\n",
    "        \n",
    "        # update mean\n",
    "        new_mean = ((old_mean * old_precision) + (self.Tis[arm_num] * self.emp_means[arm_num]))/(old_precision + self.Tis[arm_num])\n",
    "        new_precision = old_precision + 1\n",
    "        self.posteriors[arm_num] = scipy.stats.norm(loc=new_mean, scale=np.sqrt(1/new_precision))\n",
    "        return\n",
    "    \n",
    "    def step(self):\n",
    "        sample_means = [post.rvs() for post in self.posteriors]\n",
    "        It = np.argmax(sample_means)\n",
    "        self.pull(It)\n",
    "        self.update_posterior(It)\n",
    "        return\n",
    "    \n",
    "###################################################################################\n",
    "\n",
    "def run_agent(agent):\n",
    "    agent.run()\n",
    "    return agent\n",
    "\n",
    "# Problem 4.1\n",
    "T = 150000\n",
    "mus = [.1]\n",
    "for i in range(9):\n",
    "    mus.append(0)\n",
    "\n",
    "arms = [scipy.stats.norm(loc=mu) for mu in mus]\n",
    "    \n",
    "agents = [\n",
    "    UCB(T, arms, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=5, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=10, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=100, log_regret_every_n=10),\n",
    "    GaussianThompson(T, arms, prior_mean=0, prior_var=1, log_regret_every_n=10)\n",
    "]\n",
    "\n",
    "ucb, etc1, etc2, etc3, gt = Parallel(n_jobs=5)(delayed(run_agent)(agent) for agent in agents)\n",
    "\n",
    "# create dataframe\n",
    "t, r_ucb = np.array(ucb.regret_log).T\n",
    "_, r_etc1 = np.array(etc1.regret_log).T\n",
    "_, r_etc2 = np.array(etc2.regret_log).T\n",
    "_, r_etc3 = np.array(etc3.regret_log).T\n",
    "_, r_gt = np.array(gt.regret_log).T\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    't': t,\n",
    "    'UCB': r_ucb,\n",
    "    f'ETC {etc1.m}': r_etc1,\n",
    "    f'ETC {etc2.m}': r_etc2,\n",
    "    f'ETC {etc3.m}': r_etc3,\n",
    "    'GT': r_gt\n",
    "})\n",
    "\n",
    "df.to_csv('p4.1.csv')\n",
    "\n",
    "# Problem 4.2\n",
    "T = 150000\n",
    "mus = [1]\n",
    "for i in range(2,41):\n",
    "    mus.append(1-1/np.sqrt(i-1))\n",
    "print(mus)\n",
    "arms = [scipy.stats.norm(loc=mu) for mu in mus]\n",
    "    \n",
    "agents = [\n",
    "    UCB(T, arms, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=5, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=10, log_regret_every_n=10),\n",
    "    ETC(T, arms, m=100, log_regret_every_n=10),\n",
    "    GaussianThompson(T, arms, prior_mean=0, prior_var=1, log_regret_every_n=10)\n",
    "]\n",
    "\n",
    "ucb, etc1, etc2, etc3, gt = Parallel(n_jobs=5)(delayed(run_agent)(agent) for agent in agents)\n",
    "\n",
    "# create dataframe\n",
    "t, r_ucb = np.array(ucb.regret_log).T\n",
    "_, r_etc1 = np.array(etc1.regret_log).T\n",
    "_, r_etc2 = np.array(etc2.regret_log).T\n",
    "_, r_etc3 = np.array(etc3.regret_log).T\n",
    "_, r_gt = np.array(gt.regret_log).T\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    't': t,\n",
    "    'UCB': r_ucb,\n",
    "    f'ETC {etc1.m}': r_etc1,\n",
    "    f'ETC {etc2.m}': r_etc2,\n",
    "    f'ETC {etc3.m}': r_etc3,\n",
    "    'GT': r_gt\n",
    "})\n",
    "\n",
    "df.to_csv('p4.2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('p4.1.csv', index_col = 0)\n",
    "t = df['t'].values\n",
    "r_ucb = df['UCB'].values\n",
    "r_etc1 = df['ETC 5'].values\n",
    "r_etc2 = df['ETC 10'].values\n",
    "r_etc3 = df['ETC 100'].values\n",
    "r_gt = df['GT'].values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, r_ucb, c='tab:red', label='ucb')\n",
    "ax.plot(t, r_etc1, c='tab:blue', label=f'etc, m=5', alpha = .4)\n",
    "ax.plot(t, r_etc2, c='tab:blue', label=f'etc, m=10', alpha = .6)\n",
    "ax.plot(t, r_etc3, c='tab:blue', label=f'etc, m=100')\n",
    "ax.plot(t, r_gt, c='tab:green', label=f'thompson')\n",
    "ax.set_xlabel('timestep')\n",
    "ax.set_ylabel('regret')\n",
    "plt.legend()\n",
    "plt.savefig('p4.1.png', bbox_inches='tight', dpi=600)\n",
    "\n",
    "#############\n",
    "df = pd.read_csv('p4.2.csv', index_col = 0)\n",
    "t = df['t'].values\n",
    "r_ucb = df['UCB'].values\n",
    "r_etc1 = df['ETC 5'].values\n",
    "r_etc2 = df['ETC 10'].values\n",
    "r_etc3 = df['ETC 100'].values\n",
    "r_gt = df['GT'].values\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(t, r_ucb, c='tab:red', label='ucb')\n",
    "ax.plot(t, r_etc1, c='tab:blue', label=f'etc, m=5', alpha = .4)\n",
    "ax.plot(t, r_etc2, c='tab:blue', label=f'etc, m=10', alpha = .6)\n",
    "ax.plot(t, r_etc3, c='tab:blue', label=f'etc, m=100')\n",
    "ax.plot(t, r_gt, c='tab:green', label=f'thompson')\n",
    "ax.set_xlabel('timestep')\n",
    "ax.set_ylabel('regret')\n",
    "plt.legend()\n",
    "plt.savefig('p4.2.png', bbox_inches='tight', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
